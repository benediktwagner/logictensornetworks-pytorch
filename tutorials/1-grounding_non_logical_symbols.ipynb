{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grounding in Logic Tensor Networks (LTN)\n",
    "\n",
    "## Real Logic\n",
    "\n",
    "The semantics of LTN differs from the standard abstract semantics of First-order Logic (FOL) in that domains are interpreted concretely by tensors in the Real field.\n",
    "To emphasize the fact that LTN interprets symbols which are grounded on real-valued features, we use the term *grounding*, denoted by $\\mathcal{G}$, instead of interpretation. \n",
    "$\\mathcal{G}$ associates a tensor of real numbers to any term of the language, and a real number in the interval $[0,1]$ to any formula $\\phi$. \n",
    "In the rest of the tutorials, we commonly use \"tensor\" to designate \"tensor in the Real field\".\n",
    "\n",
    "The language consists of a non-logical part (the signature) and logical connectives and quantifiers.\n",
    "* **constants** denote individuals from some space of tensors $\\bigcup\\limits_{n_1 \\dots n_d \\in \\mathbb{N}^*} \\mathbb{R}^{n_1 \\times \\dots \\times n_d}$ (tensor of any rank). The individual can be pre-defined (data point) or learnable (embedding).\n",
    "* **variables** denote sequence of individuals.\n",
    "* **functions** can be any mathematical function either pre-defined or learnable. Examples of functions can be distance functions, regressors, etc. Functions can be defined using any operations in Tensorflow. They can be linear functions, Deep Neural Networks, and so forth.\n",
    "* **predicates** are represented as mathematical functions that map from some n-ary domain of individuals to a real from $[0,1]$ that can be interpreted as a truth degree. Examples of predicates can be similarity measures, classifiers, etc.\n",
    "* **connectives** -- not, and, or, implies -- are modeled using fuzzy semantics.\n",
    "* **quantifiers** are defined using aggregators.\n",
    "\n",
    "This tutorial explains how to ground constants, variables, functions and predicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logictensornetworks as ltn\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "LTN constants are grounded as some real tensor. Each constant $c$ is mapped to a point in $\\mathcal{G}(c) \\in \\bigcup\\limits_{n_1 \\dots n_d \\in \\mathbb{N}^*} \\mathbb{R}^{n_1 \\times \\dots \\times n_d}$. Notice that the objects in the domain may be tensors of any rank. A tensor of rank $0$ corresponds to a scalar, a tensor of rank $1$ to a vector, a tensor of rank $2$ to a matrix and so forth, in the usual way.  \n",
    "Here we define $\\mathcal{G}(c_1)=(2.1,3)$ and $\\mathcal{G}(c_2)=\\begin{pmatrix}\n",
    "4.2 & 3 & 2.5\\\\\n",
    "4 & -1.3 & 1.8\n",
    "\\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = ltn.constant([2.1,3])\n",
    "c2 = ltn.constant([[4.2,3,2.5],[4,-1.3,1.8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a constant can be set as learnable by using the optional argument `trainable=True`. This is useful to learn embeddings for some individuals.\n",
    "The features of the tensor will be considered as trainable parameters (learning in LTN is explained in a further notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c3 = ltn.constant([0.,0.], trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LTN constants are implemented with Tensorflow constants when non trainable, and with Tensorflow variables when trainable, with some added dynamic attributes. That means we can query the value of the constant easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2.1 3. ], shape=(2,), dtype=float32)\n",
      "[2.1 3. ]\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(c1)\n",
    "print(c1.numpy())\n",
    "print(c3)\n",
    "print(c3.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicates\n",
    "\n",
    "LTN Predicates are grounded in mappings that assign a value between zero and one to some n-ary space of input values. Predicates in LTN can be neural networks or any other function that achieves such a mapping.  \n",
    "\n",
    "There are different ways to construct a predicate in LTN:\n",
    "- the default constructor `ltn.Predicate(model)` takes in argument a `tf.keras.Model` instance; it can be used to ground any custom function (succession of operations, Deep Neural Network, ...) that return a value in $[0,1]$,\n",
    "- the lambda constructor `ltn.Predicate.Lambda(function)` takes in argument a lambda function; it is appropriate for small mathematical operations with **no trainable weights** (non-trainable function) that return a value in $[0,1]$.\n",
    "\n",
    "The following defines a predicate $P_1$ using the similarity to the point $\\vec{\\mu}=\\left<2,3\\right>$ with $\\mathcal{G}(P_1):\\vec{x}\\mapsto \\exp(-\\|\\vec{x}-\\vec{\\mu} \\|^2)$, and a predicate $P_2$ defined using a Tensorflow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = tf.constant([2.,3.])\n",
    "P1 = ltn.Predicate.Lambda(lambda x: tf.exp(-tf.norm(x-mu,axis=1)))\n",
    "\n",
    "class ModelP2(tf.keras.Model):\n",
    "    \"\"\"For more info on how to use tf.keras.Model:\n",
    "    https://www.tensorflow.org/api_docs/python/tf/keras/Model\"\"\"\n",
    "    def __init__(self):\n",
    "        super(ModelP2, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(5, activation=tf.nn.elu)\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid) # returns one value in [0,1]\n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "modelP2 = ModelP2()\n",
    "P2 = ltn.Predicate(modelP2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can easily query predicates using LTN constants and LTN variables (see further in this notebook to query using variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.9048375, shape=(), dtype=float32)\n",
      "tf.Tensor(0.035787195, shape=(), dtype=float32)\n",
      "tf.Tensor(0.019351482, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c1 = ltn.constant([2.1,3])\n",
    "c2 = ltn.constant([4.5,0.8])\n",
    "print(P1(c1))\n",
    "print(P1(c2))\n",
    "\n",
    "print(P2(c1)) # The first call of a tf model initializes its input layers.\n",
    "              # After this line, P2 is a model with inputs in R^2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:  \n",
    "- If an LTN predicate (or an LTN function) takes several inputs, e.g. $P_3(x_1,x_2)$, the arguments must be passed as a list (cf Tensorflow's conventions).\n",
    "- LTN converts inputs such that there is a \"batch\" dimension on the first axis. Therefore, most operations should work with `axis=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.56961465, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class ModelP3(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ModelP3, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(5, activation=tf.nn.elu)\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid) # returns one value in [0,1]\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x1, x2 = inputs[0], inputs[1] # multiple arguments are passed as a list\n",
    "        x = tf.concat([x1,x2],axis=1) # axis=0 is the batch axis\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "    \n",
    "P3 = ltn.Predicate(ModelP3())\n",
    "c1 = ltn.constant([2.1,3])\n",
    "c2 = ltn.constant([4.5,0.8])\n",
    "print(P3([c1,c2])) # multiple arguments are passed as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One case define trainable or non trainable 0-ary predicates (propositional variables) using `ltn.proposition`. They are grounded as a mathematical constant in $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring a trainable 0-ary predicate with initial truth value 0.3\n",
    "A = ltn.proposition(0.3, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details and useful ways to create predicates (incl. how to integrate multiclass classifiers as binary predicates), see the complementary notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "LTN functions are grounded as any mathematical function that maps $n$ individuals to one individual in the tensor domains.  \n",
    "\n",
    "There are different ways to construct an LTN function in LTN:\n",
    "- the default constructor `ltn.Function(model)` takes in argument a `tf.keras.Model` instance; it can be used to ground any custom function (succession of operations, Deep Neural Network, ...),\n",
    "- the lambda constructor `ltn.Function.Lambda(function)` takes in argument a lambda function; it is appropriate for small mathematical operations with **no weight tracking** (non-trainable function).\n",
    "\n",
    "The following defines the grounding of a function $f_1$ that computes the difference of two inputs with $\\mathcal{G}(f_1):\\vec{u},\\vec{v}\\mapsto \\vec{u} - \\vec{v}$ and a function $f_2$ that uses a deep neural network that projects a value to $\\mathbb{R}^5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = ltn.Function.Lambda(lambda args: args[0]-args[1])\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(5)\n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "    \n",
    "model_f2 = MyModel()\n",
    "f2 = ltn.Function(model_f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can easily query predicates using LTN constants and LTN variables (see further in this notebook to query using variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-2.4  2.2], shape=(2,), dtype=float32)\n",
      "tf.Tensor([ 1.2067754  -2.1816092   0.6760118   0.20462628 -1.0587809 ], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c1 = ltn.constant([2.1,3])\n",
    "c2 = ltn.constant([4.5,0.8])\n",
    "print(f1([c1,c2])) # multiple arguments are passed as a list\n",
    "print(f2(c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "LTN variables are sequences of individuals/constants from a domain. Variables are useful to write quantified statements, e.g. $\\forall x\\ P(x)$. Notice that a variable is a sequence and not a set; the same value can occur twice in the sequence.\n",
    "\n",
    "The following defines two variables $x$ and $y$ with respectively 10 and 5 individuals, sampled from normal distributions in $\\mathbb{R}^2$.  \n",
    "In LTN, variables need to be labelled (see the arguments `'x'` and `'y'` below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ltn.variable('x',np.random.normal(0.,1.,(10,2)))\n",
    "y = ltn.variable('y',np.random.normal(0.,4.,(5,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating a term/predicate with one variable of $n$ individuals yields $n$ output values, where the $i$-th output value corresponds to the term calculated with the $i$-th individual.\n",
    "\n",
    "Similarly, evaluating a term/predicate with $k$ variables $(x_1,\\dots,x_k)$, with respectively $n_1,\\dots,n_k$ individuals each, yields a result with $n_1 \\times \\dots \\times n_k$ values. The result is organized in a tensor where the first $k$ dimensions can be indexed to retrieve the outcome(s) that correspond to each variable. The tensor has a dynamically added attribute `active_doms` that tells which axis corresponds to which variable (using the label of the variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n",
      "['x', 'y']\n",
      "tf.Tensor(0.23678231, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Notice that the outcome is a 2 dimensional tensor where each cell\n",
    "# represents the satisfiability of P3 with each individual in x and in y.\n",
    "res1 = P3([x,y])\n",
    "print(res1.shape)\n",
    "print(res1.active_doms) # dynamically added attribute; tells that axis 0 corresponds to x and axis 1 to y \n",
    "print(res1[2,0]) # gives the result calculated with the 3rd individual in x and the 1st individual in y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 2)\n",
      "['x', 'y']\n",
      "tf.Tensor([ 2.8219783 -3.1634347], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Notice that the last axe(s) correspond to the dimensions of each outcome;\n",
    "# here, f2 projects to outcomes in R^2, so the outcome has one additional axis of dimension 2.\n",
    "res2 = f1([x,y])\n",
    "print(res2.shape)\n",
    "print(res2.active_doms)\n",
    "print(res2[2,0]) # gives the result calculated with the 3rd individual in x and the 1st individual in y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "['y']\n",
      "tf.Tensor(0.120424174, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c1 = ltn.constant([2.1,3])\n",
    "res3 = P3([c1,y])\n",
    "print(res3.shape) # Notice that no axis is associated to a constant.\n",
    "print(res3.active_doms)\n",
    "print(res3[0]) # gives the result calculated with c1 and the 1st individual in y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables made of trainable constants\n",
    "\n",
    "A variable can be instantiated using two different types of objects:\n",
    "- A value (numpy, python list, ...) that will be fed in a `tf.constant` (the variable refers to a new object).\n",
    "- A `tf.Tensor` instance that will be used directly as the variable (the variable refers to the same object).\n",
    "\n",
    "The latter is useful when the variable denotes a sequence of trainable constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02226352, -0.00509561], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = ltn.constant([2.1,3], trainable=True)\n",
    "c2 = ltn.constant([4.5,0.8], trainable=True)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Notice that the assignation must be done within a tf.GradientTape.\n",
    "    # Tensorflow will keep track of the gradients between c1/c2 and x.\n",
    "    # Read tutorial 3 for more details.\n",
    "    x = ltn.variable(\"x\",tf.stack([c1,c2]))\n",
    "    res = P2(x)\n",
    "tape.gradient(res,c1).numpy() # the tape keeps track of gradients between P2(x), x and c1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-tf2",
   "language": "python",
   "name": "env-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
