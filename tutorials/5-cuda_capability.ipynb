{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pytorch CUDA Computations\n",
    "\n",
    "Pytorch supports CUDA tensor types, that implement the same function as CPU\n",
    "tensors, but utilize GPUs for computation. see https://pytorch.org/docs/stable/cuda.html\n",
    "for further details.\n",
    "\n",
    "Important to note is that computation between a CUDA tensor and a CPU tensor is\n",
    "allowed and with produce an error."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LTN CUDA impementation\n",
    "\n",
    "LTN allows for constants, variables, predicates, and functions to be created\n",
    "directly on CUDA memory, by passing the GPU device as an argument (otherwise defaults to CPU)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import logictensornetworks as ltn\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1's device:  cpu\n",
      "c2's device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU device is available, and if true store the device name as python variable\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Creating a trainable embedding as a ltn constant on the GPU device (if available)\n",
    "c1 = ltn.constant(np.random.uniform(low=0.,high=1.,size=4))\n",
    "c2 = ltn.constant(np.random.uniform(low=0.,high=1.,size=4), device=device)\n",
    "\n",
    "print('c1\\'s device: ', c1.device)\n",
    "print('c2\\'s device: ', c2.device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LTN variables can take non-cuda tensors as the feed, but still be assign to the GPU.\n",
    "\n",
    "*All tensors in the feed must exist on the same device."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1's device:  cuda:0\n",
      "v2's device:  cuda:0\n",
      "v3's device:  cpu\n"
     ]
    }
   ],
   "source": [
    "c3 = ltn.constant(np.random.uniform(low=0.,high=1.,size=4))\n",
    "c4 = ltn.constant(np.random.uniform(low=0.,high=1.,size=4), device=device)\n",
    "\n",
    "v1 = ltn.variable('v1',torch.stack([c1,c3]), device=device) # feed is on CPU, but variable is on GPU\n",
    "v2 = ltn.variable('v2',torch.stack([c2,c4]), device=device) # feed is on GPU and same for variable\n",
    "v3 = ltn.variable('v2',torch.stack([c2,c4])) # feed is on GPU, but variable is not\n",
    "\n",
    "print('v1\\'s device: ', v1.device)\n",
    "print('v2\\'s device: ', v2.device)\n",
    "print('v3\\'s device: ', v3.device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A's device:  cpu\n",
      "B's device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "A = ltn.Predicate.MLP([4,4], hidden_layer_sizes=[8,8])\n",
    "B = ltn.Predicate.MLP([4,4], hidden_layer_sizes=[8,8], device=device)\n",
    "\n",
    "print('A\\'s device: ', A([c1,c3]).device)\n",
    "print('B\\'s device: ', B([c2,c4]).device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The result of ltn logical operators and qualifiers will result in\n",
    "tensors on the same device as the input."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND on cpu tensors : cpu\n",
      "AND on gpu tensors : cuda:0\n"
     ]
    }
   ],
   "source": [
    "and_luk = ltn.fuzzy_ops.And_Luk()\n",
    "\n",
    "print('AND on cpu tensors :', and_luk(c1,c3).device)\n",
    "print('AND on gpu tensors :', and_luk(c2,c4).device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}