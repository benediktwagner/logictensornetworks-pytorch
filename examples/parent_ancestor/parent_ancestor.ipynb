{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging; logging.basicConfig(level=logging.INFO)\n",
    "import torch\n",
    "import numpy as np\n",
    "import logictensornetworks as ltn\n",
    "import networkx as nx\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Parent relationships (the knowledge is assumed complete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [\"sue\", \"diana\", \"john\", \"edna\", \"paul\", \"francis\", \"john2\",\n",
    "                \"john3\", \"john4\", \"joe\", \"jennifer\", \"juliet\", \"janice\",\n",
    "                \"joey\", \"tom\", \"bonnie\", \"katie\"]\n",
    "\n",
    "parents = [\n",
    "        (\"sue\", \"diana\"),\n",
    "        (\"john\", \"diana\"),\n",
    "        (\"sue\", \"bonnie\"),\n",
    "        (\"john\", \"bonnie\"),\n",
    "        (\"sue\", \"tom\"),\n",
    "        (\"john\", \"tom\"),\n",
    "        (\"diana\", \"katie\"),\n",
    "        (\"paul\", \"katie\"),\n",
    "        (\"edna\", \"sue\"),\n",
    "        (\"john2\", \"sue\"),\n",
    "        (\"edna\", \"john3\"),\n",
    "        (\"john2\", \"john3\"),\n",
    "        (\"francis\", \"john\"),\n",
    "        (\"john4\", \"john\"),\n",
    "        (\"francis\", \"janice\"),\n",
    "        (\"john4\", \"janice\"),\n",
    "        (\"janice\", \"jennifer\"),\n",
    "        (\"joe\", \"jennifer\"),\n",
    "        (\"janice\", \"juliet\"),\n",
    "        (\"joe\", \"juliet\"),\n",
    "        (\"janice\", \"joey\"),\n",
    "        (\"joe\", \"joey\")]\n",
    "\n",
    "all_relationships = list(itertools.product(entities, repeat=2))\n",
    "not_parents = [item for item in all_relationships if item not in parents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualized in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ground Truth Parents\n",
    "# parDG_truth = nx.DiGraph(parents)\n",
    "# pos= nx.drawing.nx_agraph.graphviz_layout(parDG_truth, prog='dot')\n",
    "# nx.draw(parDG_truth,pos,with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ancestor relationships and visualization in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground Truth Ancestors\n",
    "def get_descendants(entity, DG):\n",
    "    all_d = []\n",
    "    direct_d = list(DG.successors(entity))\n",
    "    all_d += direct_d\n",
    "    for d in direct_d:\n",
    "        all_d += get_descendants(d, DG)\n",
    "    return all_d\n",
    "\n",
    "ancestors = []\n",
    "for e in entities:\n",
    "    for d in get_descendants(e, parDG_truth):\n",
    "        ancestors.append((e,d))\n",
    "\n",
    "# ancDG_truth = nx.DiGraph(ancestors)\n",
    "# pos= nx.drawing.nx_agraph.graphviz_layout(parDG_truth, prog='dot')\n",
    "# nx.draw(ancDG_truth,pos,with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTN\n",
    "\n",
    "Every individual is grounded as a trainable LTN constant in $\\mathbb{R}^2$. The grounding of the predicates `Parent` and `Ancestor` (modelled by multi-layer perceptrons) are learned at the same times as the embeddings for the individuals.\n",
    "\n",
    "We give the complete parent relationships in the knowledgebase. However, we don't give any ancestor relationships; they are to be inferred using a set of rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 4\n",
    "\n",
    "Ancestor = ltn.Predicate.MLP([embedding_size,embedding_size],hidden_layer_sizes=[8,8])\n",
    "Parent = ltn.Predicate.MLP([embedding_size,embedding_size],hidden_layer_sizes=[8,8])\n",
    "\n",
    "g_e = {\n",
    "    l: ltn.constant(np.random.uniform(low=0.,high=1.,size=embedding_size), trainable=True) \n",
    "    for l in entities\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=5),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(p=5),semantics=\"exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial sat level 0.51109\n"
     ]
    }
   ],
   "source": [
    "formula_aggregator = ltn.fuzzy_ops.Aggreg_pMeanError(p=5)\n",
    "\n",
    "# defining the theory\n",
    "#@tf.function\n",
    "def axioms():\n",
    "    # Variables created in the training loop, so tf.GradientTape\n",
    "    # keeps track of the connection with the trainable constants.\n",
    "    a = ltn.variable(\"a\",torch.stack(list(g_e.values())))\n",
    "    b = ltn.variable(\"b\",torch.stack(list(g_e.values())))\n",
    "    c = ltn.variable(\"c\",torch.stack(list(g_e.values())))\n",
    "\n",
    "    ## Complete knowledge about parent relationships.\n",
    "    ## The ancestor relationships are to be learned with these additional rules.\n",
    "    axioms = [\n",
    "        # forall pairs of individuals in the parent relationships: Parent(ancestor,child)\n",
    "        Parent([g_e[a],g_e[c]])\n",
    "        for a,c in parents\n",
    "    ] + \\\n",
    "    [\n",
    "        # forall pairs of individuals not in the parent relationships: Not(Parent([n_parent,n_child])))\n",
    "        Not(Parent([g_e[a],g_e[c]]))\n",
    "        for a,c in not_parents\n",
    "    ] + \\\n",
    "    [\n",
    "        # if a is parent of b, then a is ancestor of b\n",
    "        Forall((a,b), Implies(Parent([a,b]),Ancestor([a,b]))),\n",
    "        # parent is anti reflexive\n",
    "        Forall(a, Not(Parent([a,a]))),\n",
    "        # ancestor is anti reflexive\n",
    "        Forall(a, Not(Ancestor([a,a]))),\n",
    "        # parent is anti symmetric\n",
    "        Forall((a,b), Implies(Parent([a,b]),Not(Parent([b,a])))),\n",
    "        # if a is parent of an ancestor of c, a is an ancestor of c too\n",
    "        Forall(\n",
    "            (a,b,c),\n",
    "            Implies(And(Parent([a,b]),Ancestor([b,c])), Ancestor([a,c])),\n",
    "            p=6\n",
    "        ),\n",
    "        # if a is an ancestor of b, a is a parent of b OR a parent of an ancestor of b\n",
    "        Forall(\n",
    "            (a,b),\n",
    "            Implies(Ancestor([a,b]),\n",
    "                    Or(Parent([a,b]),\n",
    "                       Exists(c, And(Ancestor([a,c]),Parent([c,b])),p=6)\n",
    "                      )\n",
    "                   )\n",
    "        )\n",
    "    ]\n",
    "    # computing sat_level\n",
    "    axioms = torch.stack([torch.squeeze(ax) for ax in axioms])\n",
    "    sat_level = formula_aggregator(axioms, axis=0)\n",
    "    return sat_level, axioms\n",
    "\n",
    "print(\"Initial sat level %.5f\"%axioms()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_variables = list(Parent.parameters())\\\n",
    "                      +list(Ancestor.parameters())\\\n",
    "                      +list(g_e.values())\n",
    "optimizer = torch.optim.Adam(trainable_variables, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Sat Level 0.514\n",
      "Epoch 100: Sat Level 0.579\n",
      "Epoch 200: Sat Level 0.585\n",
      "Epoch 300: Sat Level 0.597\n",
      "Epoch 400: Sat Level 0.611\n",
      "Epoch 500: Sat Level 0.617\n",
      "Epoch 600: Sat Level 0.625\n",
      "Epoch 700: Sat Level 0.641\n",
      "Epoch 800: Sat Level 0.655\n",
      "Epoch 900: Sat Level 0.684\n",
      "Epoch 1000: Sat Level 0.727\n",
      "Epoch 1100: Sat Level 0.821\n",
      "Epoch 1200: Sat Level 0.918\n",
      "Epoch 1300: Sat Level 0.923\n",
      "Epoch 1400: Sat Level 0.923\n",
      "Epoch 1500: Sat Level 0.923\n",
      "Epoch 1600: Sat Level 0.924\n",
      "Epoch 1700: Sat Level 0.924\n",
      "Epoch 1800: Sat Level 0.934\n",
      "Epoch 1900: Sat Level 0.935\n",
      "Training finished at Epoch 1999 with Sat Level 0.935\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    loss_value = 1. - axioms()[0]\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    if epoch%200 == 0:\n",
    "        print(\"Epoch %d: Sat Level %.3f\"%(epoch, axioms()[0]))\n",
    "print(\"Training finished at Epoch %d with Sat Level %.3f\"%(epoch, axioms()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying additional axioms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional axioms:\n",
    "1. forall a,b,c: (Ancestor(a,b) & Parent(b,c)) -> Ancestor (a,c)\n",
    "2. forall a,b: Ancestor(a,b) -> ~Ancestor(b,a)\n",
    "3. forall a,b,c: (Parent(a,b) & Parent(b,c)) -> Ancestor(a,c)\n",
    "4. forall a,b,c: (Ancestor(a,b) & Ancestor(b,c)) -> Ancestor(a,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ltn.variable(\"a\",torch.stack(list(g_e.values())))\n",
    "b = ltn.variable(\"b\",torch.stack(list(g_e.values())))\n",
    "c = ltn.variable(\"c\",torch.stack(list(g_e.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.7676, grad_fn=<RsubBackward1>)"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Forall((a,b,c), \n",
    "       Implies(And(Ancestor([a,b]),Parent([b,c])), Ancestor([a,c]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.9729, grad_fn=<RsubBackward1>)"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Forall((a,b), \n",
    "       Implies(Ancestor([a,b]), Not(Ancestor([b,a])))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.7775, grad_fn=<RsubBackward1>)"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Forall((a,b,c),\n",
    "       Implies(And(Parent([a,b]),Parent([b,c])), Ancestor([a,c]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.7775, grad_fn=<RsubBackward1>)"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Forall((a,b,c),\n",
    "       Implies(And(Parent([a,b]),Parent([b,c])), Ancestor([a,c]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "parents_test = [\n",
    "    (e1,e2) for e1 in entities for e2 in entities\n",
    "    if (Parent([g_e[e1],g_e[e2]]).detach().numpy() > 0.5)\n",
    "]\n",
    "\n",
    "# parDG_test = nx.DiGraph(parents_test)\n",
    "# pos= nx.drawing.nx_agraph.graphviz_layout(parDG_truth, prog='dot')\n",
    "# nx.draw(parDG_test,pos,with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "ancestors_test = [\n",
    "    (e1,e2) for e1 in entities for e2 in entities\n",
    "    if (Ancestor([g_e[e1],g_e[e2]]).detach().numpy() > 0.5)\n",
    "]\n",
    "\n",
    "# ancDG_test = nx.DiGraph(ancestors_test)\n",
    "# pos= nx.drawing.nx_agraph.graphviz_layout(parDG_test, prog='dot')\n",
    "# nx.draw(ancDG_test,pos,with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.2844, grad_fn=<RsubBackward1>)"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_ancestors = [item for item in all_relationships if item not in ancestors]\n",
    "\n",
    "## 3 ##\n",
    "is_ancestor = [Ancestor([g_e[a],g_e[c]]) for a,c in ancestors]\n",
    "is_ancestor = torch.stack([torch.squeeze(ax) for ax in is_ancestor])\n",
    "formula_aggregator(is_ancestor, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.9257, grad_fn=<RsubBackward1>)"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 4 ##\n",
    "isnot_ancestor = [Not(Ancestor([g_e[a],g_e[c]])) for a,c in not_ancestors]\n",
    "isnot_ancestor = torch.stack([torch.squeeze(ax) for ax in isnot_ancestor])\n",
    "formula_aggregator(isnot_ancestor, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ True,  True,  True, False,  True,  True,  True,  True, False,  True,\n         True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n         True, False, False, False, False,  True,  True,  True,  True,  True,\n        False,  True,  True,  True,  True,  True, False, False, False, False,\n         True,  True,  True,  True,  True,  True])"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_ancestor>0.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7391304347826086"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(is_ancestor.detach().numpy()>0.5)/46"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(isnot_ancestor.detach().numpy()>0.5)/243"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}