{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Single-Label classification\n",
    "\n",
    "The natural extension of binary classification is a multi-class classification task.\n",
    "We first approach multi-class single-label classification, which makes the assumption that each example is assigned to one and only one label.\n",
    "\n",
    "We use the *Iris flower* data set, which consists of a classification into three mutually-exclusive classes; call these $A$, $B$ and $C$.\n",
    "\n",
    "While one could train three unary predicates $A(x)$, $B(x)$ and $C(x)$, it turns out to be more effective if this problem is modelled by a single binary predicate $P(x,l)$, where $l$ is a variable denoting a multi-class label, in this case classes $A$, $B$ or $C$.\n",
    "- This syntax allows one to write statements quantifying over the classes, e.g. $\\forall x ( \\exists l ( P(x,l)))$.\n",
    "- Since the classes are mutually-exclusive in this case, the output layer of the $\\mathtt{MLP}$ representing $P(x,l)$ will be a $\\mathtt{softmax}$ layer, instead of a $\\mathtt{sigmoid}$ function, to learn the probability of $A$, $B$ and $C$. This avoids writing additional constraints $\\lnot (A(x) \\land B(x))$, $\\lnot (A(x) \\land C(x))$, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging; logging.basicConfig(level=logging.INFO)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import logictensornetworks as ltn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Load the iris dataset: 50 samples from each of three species of iris flowers (setosa, virginica, versicolor), measured with four features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width  species\n",
      "0           6.4          2.8           5.6          2.2        2\n",
      "1           5.0          2.3           3.3          1.0        1\n",
      "2           4.9          2.5           4.5          1.7        2\n",
      "3           4.9          3.1           1.5          0.1        0\n",
      "4           5.7          3.8           1.7          0.3        0\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"iris_training.csv\")\n",
    "df_test = pd.read_csv(\"iris_test.csv\")\n",
    "print(df_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = df_train.pop(\"species\")\n",
    "labels_test = df_test.pop(\"species\")\n",
    "batch_size = 64\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((df_train,labels_train)).batch(batch_size)\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((df_test,labels_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTN\n",
    "\n",
    "Predicate with softmax `P(x,class)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    \"\"\"Model that returns logits.\"\"\"\n",
    "    def __init__(self, n_classes, hidden_layer_sizes=(16,16,8)):\n",
    "        super(MLP, self).__init__()\n",
    "        self.denses = [tf.keras.layers.Dense(s, activation=\"elu\") for s in hidden_layer_sizes]\n",
    "        self.dense_class = tf.keras.layers.Dense(n_classes)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        for dense in self.denses:\n",
    "            x = dense(x)\n",
    "            x = self.dropout(x, training=training)\n",
    "        return self.dense_class(x)\n",
    "\n",
    "logits_model = MLP(4)\n",
    "p = ltn.Predicate(ltn.utils.LogitsToPredicateModel(logits_model,single_label=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants to index/iterate on the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_A = ltn.constant(0)\n",
    "class_B = ltn.constant(1)\n",
    "class_C = ltn.constant(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operators and axioms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=2),semantics=\"forall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_aggregator = ltn.fuzzy_ops.Aggreg_pMeanError(p=2)\n",
    "\n",
    "@tf.function\n",
    "def axioms(features, labels, training=False):\n",
    "    x_A = ltn.variable(\"x_A\",features[labels==0])\n",
    "    x_B = ltn.variable(\"x_B\",features[labels==1])\n",
    "    x_C = ltn.variable(\"x_C\",features[labels==2])\n",
    "    axioms = [\n",
    "        Forall(x_A,p([x_A,class_A],training=training)),\n",
    "        Forall(x_B,p([x_B,class_B],training=training)),\n",
    "        Forall(x_C,p([x_C,class_C],training=training))\n",
    "    ]\n",
    "    axioms = tf.stack(axioms)\n",
    "    sat_level = formula_aggregator(axioms)\n",
    "    return sat_level, axioms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize all layers and the static graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial sat level 0.18089\n"
     ]
    }
   ],
   "source": [
    "for features, labels in ds_test:\n",
    "    print(\"Initial sat level %.5f\"%axioms(features,labels)[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Define the metrics. While training, we measure:\n",
    "1. The level of satisfiability of the Knowledge Base of the training data.\n",
    "1. The level of satisfiability of the Knowledge Base of the test data.\n",
    "3. The training accuracy.\n",
    "4. The test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "    'train_sat_kb': tf.keras.metrics.Mean(name='train_sat_kb'),\n",
    "    'test_sat_kb': tf.keras.metrics.Mean(name='test_sat_kb'),\n",
    "    'train_accuracy': tf.keras.metrics.CategoricalAccuracy(name=\"train_accuracy\"),\n",
    "    'test_accuracy': tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training and test step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "@tf.function\n",
    "def train_step(features, labels):\n",
    "    # sat and update\n",
    "    with tf.GradientTape() as tape:\n",
    "        sat = axioms(features, labels, training=True)[0]\n",
    "        loss = 1.-sat\n",
    "    gradients = tape.gradient(loss, p.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, p.trainable_variables))\n",
    "    sat = axioms(features, labels)[0] # compute sat without dropout\n",
    "    metrics_dict['train_sat_kb'](sat)\n",
    "    # accuracy\n",
    "    predictions = logits_model(features)\n",
    "    metrics_dict['train_accuracy'](tf.one_hot(labels,3),predictions)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(features, labels):\n",
    "    # sat\n",
    "    sat = axioms(features, labels)[0]\n",
    "    metrics_dict['test_sat_kb'](sat)\n",
    "    # accuracy\n",
    "    predictions = logits_model(features)\n",
    "    metrics_dict['test_accuracy'](tf.one_hot(labels,3),predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer mlp is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function axioms at 0x7f28e05c1040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 8 calls to <function axioms at 0x7f28e05c1040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_sat_kb: 0.1825, test_sat_kb: 0.1825, train_accuracy: 0.3500, test_accuracy: 0.2667\n",
      "Epoch 20, train_sat_kb: 0.3672, test_sat_kb: 0.3700, train_accuracy: 0.6333, test_accuracy: 0.7333\n",
      "Epoch 40, train_sat_kb: 0.5091, test_sat_kb: 0.5074, train_accuracy: 0.6500, test_accuracy: 0.7333\n",
      "Epoch 60, train_sat_kb: 0.5793, test_sat_kb: 0.5756, train_accuracy: 0.8250, test_accuracy: 0.6333\n",
      "Epoch 80, train_sat_kb: 0.6117, test_sat_kb: 0.6080, train_accuracy: 0.8583, test_accuracy: 0.8000\n",
      "Epoch 100, train_sat_kb: 0.6283, test_sat_kb: 0.6250, train_accuracy: 0.9667, test_accuracy: 1.0000\n",
      "Epoch 120, train_sat_kb: 0.6492, test_sat_kb: 0.6467, train_accuracy: 0.9750, test_accuracy: 0.9667\n",
      "Epoch 140, train_sat_kb: 0.6770, test_sat_kb: 0.6763, train_accuracy: 0.9833, test_accuracy: 0.9667\n",
      "Epoch 160, train_sat_kb: 0.7116, test_sat_kb: 0.7147, train_accuracy: 0.9833, test_accuracy: 0.9667\n",
      "Epoch 180, train_sat_kb: 0.7495, test_sat_kb: 0.7558, train_accuracy: 0.9917, test_accuracy: 0.9667\n",
      "Epoch 200, train_sat_kb: 0.7876, test_sat_kb: 0.7993, train_accuracy: 0.9833, test_accuracy: 0.9667\n",
      "Epoch 220, train_sat_kb: 0.8126, test_sat_kb: 0.8246, train_accuracy: 0.9833, test_accuracy: 0.9667\n",
      "Epoch 240, train_sat_kb: 0.8309, test_sat_kb: 0.8422, train_accuracy: 0.9750, test_accuracy: 0.9667\n",
      "Epoch 260, train_sat_kb: 0.8449, test_sat_kb: 0.8534, train_accuracy: 0.9833, test_accuracy: 0.9667\n",
      "Epoch 280, train_sat_kb: 0.8554, test_sat_kb: 0.8593, train_accuracy: 0.9750, test_accuracy: 0.9667\n",
      "Epoch 300, train_sat_kb: 0.8676, test_sat_kb: 0.8674, train_accuracy: 0.9833, test_accuracy: 0.9667\n",
      "Epoch 320, train_sat_kb: 0.8626, test_sat_kb: 0.8628, train_accuracy: 0.9750, test_accuracy: 0.9667\n",
      "Epoch 340, train_sat_kb: 0.8700, test_sat_kb: 0.8653, train_accuracy: 0.9750, test_accuracy: 0.9667\n",
      "Epoch 360, train_sat_kb: 0.8774, test_sat_kb: 0.8683, train_accuracy: 0.9833, test_accuracy: 0.9667\n",
      "Epoch 380, train_sat_kb: 0.8835, test_sat_kb: 0.8695, train_accuracy: 0.9833, test_accuracy: 0.9667\n",
      "Epoch 400, train_sat_kb: 0.8856, test_sat_kb: 0.8696, train_accuracy: 0.9833, test_accuracy: 0.9667\n",
      "Epoch 420, train_sat_kb: 0.8835, test_sat_kb: 0.8685, train_accuracy: 0.9750, test_accuracy: 0.9667\n",
      "Epoch 440, train_sat_kb: 0.8882, test_sat_kb: 0.8689, train_accuracy: 0.9833, test_accuracy: 0.9667\n",
      "Epoch 460, train_sat_kb: 0.8885, test_sat_kb: 0.8668, train_accuracy: 0.9750, test_accuracy: 0.9667\n",
      "Epoch 480, train_sat_kb: 0.8934, test_sat_kb: 0.8683, train_accuracy: 0.9833, test_accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "import commons\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "commons.train(\n",
    "    EPOCHS,\n",
    "    metrics_dict,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    train_step,\n",
    "    test_step,\n",
    "    csv_path=\"iris_results.csv\",\n",
    "    track_metrics=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-tf2",
   "language": "python",
   "name": "env-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
