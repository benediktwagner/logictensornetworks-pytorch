{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Multi-Label classification\n",
    "\n",
    "We now turn to multi-label classification, whereby multiple labels can be assigned to each example. \n",
    "As a first example of the reach of LTNs, we shall see how the previous example can be extended naturally using LTN to account for multiple labels, not always a trivial extension for most ML algorithms.    \n",
    "\n",
    "The standard approach to the multi-label problem is to provide explicit negative examples for each class. By contrast, LTN can use background knowledge to relate classes directly to each other, thus becoming a powerful tool in the case of the multi-label problem when typically the labelled data is scarce.\n",
    "\n",
    "We use the *Leptograpsus crabs* data set consisting of 200 examples of 5 morphological measurements of 50 crabs. The task is to classify the crabs according to their colour and sex. There are four labels: blue, orange, male and female. \n",
    "\n",
    "The colour labels are mutually-exclusive, and so are the labels for sex. LTN will be used to specify such information logically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging; logging.basicConfig(level=logging.INFO)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logictensornetworks as ltn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Crabs dataset from: http://www.stats.ox.ac.uk/pub/PRNN/\n",
    "\n",
    "The crabs data frame has 200 rows and 8 columns, describing 5 morphological measurements on 50 crabs each of two colour forms and both sexes, of the species Leptograpsus variegatus collected at Fremantle, W. Australia.\n",
    "\n",
    "- Multi-class: Male, Female, Blue, Orange.\n",
    "- Multi-label: Only Male-Female and Blue-Orange are mutually exclusive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sp sex  index    FL    RW    CL    CW    BD\n",
      "171  O   F     22  17.5  14.4  34.5  39.0  16.0\n",
      "149  O   M     50  23.1  15.7  47.6  52.8  21.6\n",
      "169  O   F     20  17.1  14.5  33.1  37.2  14.6\n",
      "139  O   M     40  19.4  14.4  39.8  44.3  17.9\n",
      "102  O   M      3  10.7   8.6  20.7  22.7   9.2\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"crabs.dat\",sep=\" \", skipinitialspace=True)\n",
    "df = df.sample(frac=1) #shuffle\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 160 samples for training and 40 samples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['FL','RW','CL','CW','BD']]\n",
    "labels_sex = df['sex']\n",
    "labels_color = df['sp']\n",
    "\n",
    "batch_size=64\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((features[:160],labels_sex[:160],labels_color[:160])).batch(batch_size)\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((features[160:],labels_sex[160:],labels_color[160:])).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTN\n",
    "\n",
    "### Predicate\n",
    "\n",
    "| index | class | \n",
    "| --- | --- |\n",
    "| 0 | Male |\n",
    "| 1 | Female |\n",
    "| 2 | Blue |\n",
    "| 3 | Orange |\n",
    "\n",
    "Let's note that, since the classes are not mutually exclusive, the last layer of the model will be a `sigmoid` and not a `softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    \"\"\"Model that returns logits.\"\"\"\n",
    "    def __init__(self, n_classes, hidden_layer_sizes=(16,16,8)):\n",
    "        super(MLP, self).__init__()\n",
    "        self.denses = [tf.keras.layers.Dense(s, activation=\"elu\") for s in hidden_layer_sizes]\n",
    "        self.dense_class = tf.keras.layers.Dense(n_classes)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for dense in self.denses:\n",
    "            x = dense(x)\n",
    "        return self.dense_class(x)\n",
    "\n",
    "logits_model = MLP(4)\n",
    "p = ltn.Predicate(ltn.utils.LogitsToPredicateModel(logits_model,single_label=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants to index the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_male = ltn.constant(0)\n",
    "class_female = ltn.constant(1)\n",
    "class_blue = ltn.constant(2)\n",
    "class_orange = ltn.constant(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Axioms\n",
    "\n",
    "```\n",
    "forall x_blue: C(x_blue,blue)\n",
    "forall x_orange: C(x_orange,orange)\n",
    "forall x_male: C(x_male,male)\n",
    "forall x_female: C(x_female,female)\n",
    "forall x: ~(C(x,male) & C(x,female))\n",
    "forall x: ~(C(x,blue) & C(x,orange))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=2),semantics=\"forall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_aggregator = ltn.fuzzy_ops.Aggreg_pMeanError(p=2)\n",
    "\n",
    "@tf.function\n",
    "def axioms(features,labels_sex,labels_color):\n",
    "    x = ltn.variable(\"x\",features)\n",
    "    x_blue = ltn.variable(\"x_blue\",features[labels_color==\"B\"])\n",
    "    x_orange = ltn.variable(\"x_orange\",features[labels_color==\"O\"])\n",
    "    x_male = ltn.variable(\"x_blue\",features[labels_sex==\"M\"])\n",
    "    x_female = ltn.variable(\"x_blue\",features[labels_sex==\"F\"])\n",
    "    axioms = [\n",
    "        Forall(x_blue, p([x_blue,class_blue])),\n",
    "        Forall(x_orange, p([x_orange,class_orange])),\n",
    "        Forall(x_male, p([x_male,class_male])),\n",
    "        Forall(x_female, p([x_female,class_female])),\n",
    "        Forall(x,Not(And(p([x,class_blue]),p([x,class_orange])))),\n",
    "        Forall(x,Not(And(p([x,class_male]),p([x,class_female]))))\n",
    "    ]\n",
    "    axioms = tf.stack(axioms)\n",
    "    sat_level = formula_aggregator(axioms)\n",
    "    return sat_level, axioms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize all layers and the static graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial sat level 0.42693\n"
     ]
    }
   ],
   "source": [
    "for features, labels_sex, labels_color in ds_train:\n",
    "    print(\"Initial sat level %.5f\"%axioms(features,labels_sex,labels_color)[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Define the metrics. While training, we measure:\n",
    " 1. The level of satisfiability of the Knowledge Base of the training data.\n",
    " 1. The level of satisfiability of the Knowledge Base of the test data.\n",
    " 3. The training accuracy.\n",
    " 4. The test accuracy.\n",
    " 5. The level of satisfiability of a formula $\\phi_1$ we expect to be true. \n",
    "       `forall x (p(x,blue)->~p(x,orange))` (every blue crab cannot be orange and vice-versa)\n",
    " 6. The level of satisfiability of a formula $\\phi_2$ we expect to be false.\n",
    "       `forall x (p(x,blue)->p(x,orange))` (every blue crab is also orange)\n",
    " 7. The level of satisfiability of a formula $\\phi_3$ we expect to be false. \n",
    "       `forall x (p(x,blue)->p(x,male))` (every blue crab is male)\n",
    "       \n",
    "       \n",
    "For the last 3 queries, we use $p=5$ when approximating the universal quantifier. A higher $p$ denotes a stricter universal quantification with a stronger focus on outliers (see turorial on operators for more details).\n",
    "Training should usually not focus on outliers, as optimizers would struggle to generalize and tend to get stuck in local minima. However, when querying $\\phi_1$,$\\phi_2$,$\\phi_3$, we wish to be more careful about the interpretation of our statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "    'train_sat_kb': tf.keras.metrics.Mean(name='train_sat_kb'),\n",
    "    'test_sat_kb': tf.keras.metrics.Mean(name='test_sat_kb'),\n",
    "    'train_accuracy': tf.keras.metrics.Mean(name=\"train_accuracy\"),\n",
    "    'test_accuracy': tf.keras.metrics.Mean(name=\"test_accuracy\"),\n",
    "    'test_sat_phi1': tf.keras.metrics.Mean(name='test_sat_phi1'),\n",
    "    'test_sat_phi2': tf.keras.metrics.Mean(name='test_sat_phi2'),\n",
    "    'test_sat_phi3': tf.keras.metrics.Mean(name='test_sat_phi3')\n",
    "}\n",
    "\n",
    "@tf.function()\n",
    "def phi1(features):\n",
    "    x = ltn.variable(\"x\",features)\n",
    "    return Forall(x, Implies(p([x,class_blue]),Not(p([x,class_orange]))),p=5)\n",
    "\n",
    "@tf.function()\n",
    "def phi2(features):\n",
    "    x = ltn.variable(\"x\",features)\n",
    "    return Forall(x, Implies(p([x,class_blue]),p([x,class_orange])),p=5)\n",
    "\n",
    "@tf.function()\n",
    "def phi3(features):\n",
    "    x = ltn.variable(\"x\",features)\n",
    "    return Forall(x, Implies(p([x,class_blue]),p([x,class_male])),p=5)\n",
    "\n",
    "def multilabel_hamming_loss(y_true, y_pred, threshold=0.5,from_logits=False):\n",
    "    if from_logits:\n",
    "        y_pred = tf.math.sigmoid(y_pred)\n",
    "    y_pred = y_pred > threshold\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_pred = tf.cast(y_pred, tf.int32)\n",
    "    nonzero = tf.cast(tf.math.count_nonzero(y_true-y_pred,axis=-1),tf.float32)\n",
    "    return nonzero/y_true.get_shape()[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "@tf.function\n",
    "def train_step(features, labels_sex, labels_color):\n",
    "    # sat and update\n",
    "    with tf.GradientTape() as tape:\n",
    "        sat = axioms(features, labels_sex, labels_color)[0]\n",
    "        loss = 1.-sat\n",
    "    gradients = tape.gradient(loss, p.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, p.trainable_variables))\n",
    "    metrics_dict['train_sat_kb'](sat)\n",
    "    # accuracy\n",
    "    predictions = logits_model(features)\n",
    "    labels_male = (labels_sex == \"M\")\n",
    "    labels_female = (labels_sex == \"F\")\n",
    "    labels_blue = (labels_color == \"B\")\n",
    "    labels_orange = (labels_color == \"O\")\n",
    "    onehot = tf.stack([labels_male,labels_female,labels_blue,labels_orange],axis=-1)\n",
    "    metrics_dict['train_accuracy'](1-multilabel_hamming_loss(onehot,predictions,from_logits=True))\n",
    "    \n",
    "@tf.function\n",
    "def test_step(features, labels_sex, labels_color):\n",
    "    # sat\n",
    "    sat_kb = axioms(features, labels_sex, labels_color)[0]\n",
    "    metrics_dict['test_sat_kb'](sat_kb)\n",
    "    sat_phi1 = phi1(features)\n",
    "    metrics_dict['test_sat_phi1'](sat_phi1)\n",
    "    sat_phi2 = phi2(features)\n",
    "    metrics_dict['test_sat_phi2'](sat_phi2)\n",
    "    sat_phi3 = phi3(features)\n",
    "    metrics_dict['test_sat_phi3'](sat_phi3)\n",
    "    # accuracy\n",
    "    predictions = logits_model(features)\n",
    "    labels_male = (labels_sex == \"M\")\n",
    "    labels_female = (labels_sex == \"F\")\n",
    "    labels_blue = (labels_color == \"B\")\n",
    "    labels_orange = (labels_color == \"O\")\n",
    "    onehot = tf.stack([labels_male,labels_female,labels_blue,labels_orange],axis=-1)\n",
    "    metrics_dict['test_accuracy'](1-multilabel_hamming_loss(onehot,predictions,from_logits=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer mlp is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_sat_kb: 0.4271, test_sat_kb: 0.4286, train_accuracy: 0.5031, test_accuracy: 0.4875, test_sat_phi1: 0.9629, test_sat_phi2: 0.0090, test_sat_phi3: 0.9907\n",
      "Epoch 20, train_sat_kb: 0.6164, test_sat_kb: 0.6198, train_accuracy: 0.5156, test_accuracy: 0.5188, test_sat_phi1: 0.5442, test_sat_phi2: 0.6940, test_sat_phi3: 0.8457\n",
      "Epoch 40, train_sat_kb: 0.6655, test_sat_kb: 0.6696, train_accuracy: 0.6438, test_accuracy: 0.6500, test_sat_phi1: 0.6433, test_sat_phi2: 0.5946, test_sat_phi3: 0.7717\n",
      "Epoch 60, train_sat_kb: 0.7128, test_sat_kb: 0.7123, train_accuracy: 0.8219, test_accuracy: 0.7875, test_sat_phi1: 0.7067, test_sat_phi2: 0.4759, test_sat_phi3: 0.6103\n",
      "Epoch 80, train_sat_kb: 0.7842, test_sat_kb: 0.7824, train_accuracy: 0.9375, test_accuracy: 0.9125, test_sat_phi1: 0.7752, test_sat_phi2: 0.2827, test_sat_phi3: 0.4770\n",
      "Epoch 100, train_sat_kb: 0.8409, test_sat_kb: 0.8335, train_accuracy: 0.9688, test_accuracy: 0.9500, test_sat_phi1: 0.8399, test_sat_phi2: 0.2249, test_sat_phi3: 0.4327\n",
      "Epoch 120, train_sat_kb: 0.8697, test_sat_kb: 0.8625, train_accuracy: 0.9781, test_accuracy: 0.9688, test_sat_phi1: 0.8735, test_sat_phi2: 0.2014, test_sat_phi3: 0.4083\n",
      "Epoch 140, train_sat_kb: 0.8876, test_sat_kb: 0.8787, train_accuracy: 0.9781, test_accuracy: 0.9750, test_sat_phi1: 0.8936, test_sat_phi2: 0.1887, test_sat_phi3: 0.3954\n",
      "Epoch 160, train_sat_kb: 0.8995, test_sat_kb: 0.8891, train_accuracy: 0.9797, test_accuracy: 0.9812, test_sat_phi1: 0.9082, test_sat_phi2: 0.1806, test_sat_phi3: 0.3873\n",
      "Epoch 180, train_sat_kb: 0.9090, test_sat_kb: 0.8987, train_accuracy: 0.9891, test_accuracy: 0.9875, test_sat_phi1: 0.9211, test_sat_phi2: 0.1750, test_sat_phi3: 0.3811\n"
     ]
    }
   ],
   "source": [
    "import commons\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "commons.train(\n",
    "    EPOCHS,\n",
    "    metrics_dict,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    train_step,\n",
    "    test_step,\n",
    "    csv_path=\"crabs_results.csv\",\n",
    "    track_metrics=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-tf2",
   "language": "python",
   "name": "env-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
