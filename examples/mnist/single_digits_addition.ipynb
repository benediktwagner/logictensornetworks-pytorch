{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Addition Problem\n",
    "\n",
    "Consider a task where one needs to learn a classifier $\\mathtt{addition(X,Y,N)}$ where $\\mathtt{X}$ and $\\mathtt{Y}$ are images of digits (the MNIST data set will be used), and $\\mathtt{N}$ is a natural number corresponding to the sum of these digits. The classifier should return an estimate of the validity of the addition ($0$ is invalid, $1$ is valid). \n",
    "\n",
    "For instance, if $\\mathtt{X}$ is an image of a 0 and $\\mathtt{Y}$ is an image of a 9:\n",
    "- if $\\mathtt{N} = 9$, then the addition is valid; \n",
    "- if $\\mathtt{N} = 4$, then the addition is not valid. \n",
    "\n",
    "A natural approach is to seek to first 1) learn a single digit classifier, then 2) benefit from knowledge readily available about the properties of addition.\n",
    "For instance, suppose that a predicate $\\mathrm{digit}(x,d)$ gives the likelihood of an image $x$ being of digit $d$, one could query with LTN:    \n",
    "$$\n",
    "\\exists d_1,d_2 : d_1+d_2= \\mathtt{N} \\ (\\mathrm{digit}(\\mathtt{X},d_1)\\land \\mathrm{digit}(\\mathtt{Y},d_2))\n",
    "$$\n",
    "and use the satisfaction of this query as the output of $\\mathtt{addition(X,Y,N)}$ .\n",
    "\n",
    "\n",
    "The challenge is the following:\n",
    "- We provide, in the data, pairs of images $\\mathtt{X}$, $\\mathtt{Y}$ and the result of the addition $\\mathtt{N}$ (final label),\n",
    "- We do **not** provide the intermediate labels, the correct digits for $d_1$, $d_2$.\n",
    "\n",
    "Regardless, it is possible to use the equation above as background knowledge to train $\\mathrm{digit}$ with LTN.\n",
    "In contrast, a standard neural network baseline cannot incorporate such intermediate components as nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import logictensornetworks as ltn\n",
    "import baselines, data, commons\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Dataset of images for the digits X and Y, and their label Z s.t. X+Y=Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result label is 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_train, ds_test = data.get_mnist_op_dataset(\n",
    "        count_train=3000,\n",
    "        count_test=1000,\n",
    "        buffer_size=3000,\n",
    "        batch_size=16,\n",
    "        n_operands=2,\n",
    "        op=lambda args: args[0]+args[1])\n",
    "\n",
    "# Visualize one example\n",
    "x, y, z = next(ds_train.as_numpy_iterator())\n",
    "plt.subplot(121)\n",
    "plt.imshow(x[0][:,:,0])\n",
    "plt.subplot(122)\n",
    "plt.imshow(y[0][:,:,0])\n",
    "print(\"Result label is %i\" % z[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_model = baselines.SingleDigit()\n",
    "Digit = ltn.Predicate(ltn.utils.LogitsToPredicateModel(logits_model))\n",
    "\n",
    "d1 = ltn.variable(\"digits1\", range(10))\n",
    "d2 = ltn.variable(\"digits2\", range(10))\n",
    "\n",
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(),semantics=\"exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `Diag`: when grounding $x$,$y$,$n$ with three sequences of values, the $i$-th examples of each variable are matching. \n",
    "That is, `(images_x[i],images_y[i],labels[i])` is a tuple from our dataset of valid additions.\n",
    "Using the diagonal quantification, LTN aggregates pairs of images and their corresponding result, rather than any combination of images and results. \n",
    "    \n",
    "Notice also the guarded quantification: by quantifying only on the \"intermediate labels\" (not given during training) that could add up to the result label (given during training), we incorporate symbolic information into the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.010578215>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def axioms(images_x, images_y, labels_z, p_schedule=tf.constant(2.)):\n",
    "    images_x = ltn.variable(\"x\", images_x)\n",
    "    images_y = ltn.variable(\"y\", images_y)\n",
    "    labels_z = ltn.variable(\"z\", labels_z)\n",
    "    return Forall(\n",
    "            ltn.diag(images_x,images_y,labels_z),\n",
    "            Exists(\n",
    "                (d1,d2),\n",
    "                And(Digit([images_x,d1]),Digit([images_y,d2])),\n",
    "                mask_vars=[labels_z,d2,d1],\n",
    "                mask_fn=lambda vars: tf.equal(vars[0],vars[1]+vars[2]),\n",
    "                p=p_schedule\n",
    "            ),\n",
    "            p=2\n",
    "        )\n",
    "\n",
    "images_x, images_y, labels_z = next(ds_train.as_numpy_iterator())\n",
    "axioms(images_x, images_y, labels_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer, training steps and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "metrics_dict = {\n",
    "    'train_loss': tf.keras.metrics.Mean(name=\"train_loss\"),\n",
    "    'train_accuracy': tf.keras.metrics.Mean(name=\"train_accuracy\"),\n",
    "    'test_loss': tf.keras.metrics.Mean(name=\"test_loss\"),\n",
    "    'test_accuracy': tf.keras.metrics.Mean(name=\"test_accuracy\")    \n",
    "}\n",
    "\n",
    "@tf.function\n",
    "def train_step(images_x, images_y, labels_z, **parameters):\n",
    "    # loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    gradients = tape.gradient(loss, logits_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, logits_model.trainable_variables))\n",
    "    metrics_dict['train_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model(images_x),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model(images_y),axis=-1)\n",
    "    predictions_z = predictions_x + predictions_y\n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['train_accuracy'](tf.reduce_mean(tf.cast(match,tf.float32)))\n",
    "    \n",
    "@tf.function\n",
    "def test_step(images_x, images_y, labels_z, **parameters):\n",
    "    # loss\n",
    "    loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    metrics_dict['test_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model(images_x),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model(images_y),axis=-1)\n",
    "    predictions_z = predictions_x + predictions_y\n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['test_accuracy'](tf.reduce_mean(tf.cast(match,tf.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "scheduled_parameters = defaultdict(lambda: {})\n",
    "for epoch in range(0,4):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(1.)}\n",
    "for epoch in range(4,8):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(2.)}\n",
    "for epoch in range(8,12):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(4.)}\n",
    "for epoch in range(12,20):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(6.)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer single_digit is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 0, train_loss: 0.9285, train_accuracy: 0.4328, test_loss: 0.8680, test_accuracy: 0.7242\n",
      "Epoch 1, train_loss: 0.8546, train_accuracy: 0.8457, test_loss: 0.8525, test_accuracy: 0.7986\n",
      "Epoch 2, train_loss: 0.8439, train_accuracy: 0.8986, test_loss: 0.8480, test_accuracy: 0.8274\n",
      "Epoch 3, train_loss: 0.8398, train_accuracy: 0.9176, test_loss: 0.8439, test_accuracy: 0.8522\n",
      "Epoch 4, train_loss: 0.6526, train_accuracy: 0.9109, test_loss: 0.6660, test_accuracy: 0.8462\n",
      "Epoch 5, train_loss: 0.6326, train_accuracy: 0.9378, test_loss: 0.6550, test_accuracy: 0.8690\n",
      "Epoch 6, train_loss: 0.6262, train_accuracy: 0.9478, test_loss: 0.6438, test_accuracy: 0.8909\n",
      "Epoch 7, train_loss: 0.6217, train_accuracy: 0.9564, test_loss: 0.6360, test_accuracy: 0.9087\n",
      "Epoch 8, train_loss: 0.4358, train_accuracy: 0.9458, test_loss: 0.4664, test_accuracy: 0.8938\n",
      "Epoch 9, train_loss: 0.4215, train_accuracy: 0.9571, test_loss: 0.4749, test_accuracy: 0.8829\n",
      "Epoch 10, train_loss: 0.4189, train_accuracy: 0.9578, test_loss: 0.4601, test_accuracy: 0.9018\n",
      "Epoch 11, train_loss: 0.4076, train_accuracy: 0.9688, test_loss: 0.4645, test_accuracy: 0.8929\n",
      "Epoch 12, train_loss: 0.3144, train_accuracy: 0.9704, test_loss: 0.3822, test_accuracy: 0.9048\n",
      "Epoch 13, train_loss: 0.3301, train_accuracy: 0.9624, test_loss: 0.3968, test_accuracy: 0.8929\n",
      "Epoch 14, train_loss: 0.3165, train_accuracy: 0.9694, test_loss: 0.3711, test_accuracy: 0.9157\n",
      "Epoch 15, train_loss: 0.3114, train_accuracy: 0.9724, test_loss: 0.3927, test_accuracy: 0.8998\n",
      "Epoch 16, train_loss: 0.3132, train_accuracy: 0.9724, test_loss: 0.3572, test_accuracy: 0.9276\n",
      "Epoch 17, train_loss: 0.3107, train_accuracy: 0.9734, test_loss: 0.3779, test_accuracy: 0.9127\n",
      "Epoch 18, train_loss: 0.3082, train_accuracy: 0.9734, test_loss: 0.3939, test_accuracy: 0.8988\n",
      "Epoch 19, train_loss: 0.3036, train_accuracy: 0.9761, test_loss: 0.3583, test_accuracy: 0.9246\n"
     ]
    }
   ],
   "source": [
    "commons.train(\n",
    "    20,\n",
    "    metrics_dict,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    train_step,\n",
    "    test_step,\n",
    "    scheduled_parameters=scheduled_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-tf2",
   "language": "python",
   "name": "env-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
